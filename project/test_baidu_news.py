#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æµ‹è¯•ç™¾åº¦æ–°é—»é¡µé¢ç»“æ„ - è¯¦ç»†ç‰ˆæœ¬
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import brotli

def test_baidu_news(keyword="ç§‘æŠ€"):
    """æµ‹è¯•ç™¾åº¦æ–°é—»é¡µé¢ç»“æ„ - è¯¦ç»†ç‰ˆæœ¬"""
    # å®šä¹‰ç™¾åº¦æ–°é—»æœç´¢URLå’Œå‚æ•°
    base_url = "https://www.baidu.com/s"
    params = {
        "rtt": "1",  # å®æ—¶æ’åº
        "bsst": "1",
        "cl": "2",  # æ–°é—»ç±»å‹
        "tn": "news",
        "rsv_dl": "ns_pc",
        "word": keyword,  # è®©requestsè‡ªåŠ¨å¤„ç†URLç¼–ç 
        "pn": 0  # ç¬¬ä¸€é¡µ
    }

    # å®šä¹‰è¯·æ±‚å¤´
    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-encoding": "gzip, deflate, br, zstd",
        "accept-language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
        "cache-control": "max-age=0",
        "connection": "keep-alive",
        "host": "www.baidu.com",
        "referer": "https://news.baidu.com/",
        "sec-ch-ua": '"Chromium";v="142", "Microsoft Edge";v="142", "Not_A Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "same-site",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0"
    }

    try:
        print(f"\nå¼€å§‹è·å–ç™¾åº¦æ–°é—»é¡µé¢ï¼ˆå…³é”®å­—: {keyword}ï¼‰...")
        
        # å‘é€è¯·æ±‚
        response = requests.get(
            base_url,
            params=params,
            headers=headers,
            timeout=10
        )
        response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        
        print(f"âœ… è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        print(f"âœ… å†…å®¹ç±»å‹: {response.headers.get('content-type')}")
        print(f"âœ… åŸå§‹ç¼–ç : {response.encoding}")
        
        # å¤„ç†å‹ç¼©å†…å®¹
        content = response.text
        
        print(f"âœ… é€šè¿‡response.textæˆåŠŸè§£æ")
        soup = BeautifulSoup(content, 'html.parser')
        print(f"âœ… é¡µé¢æ ‡é¢˜: {soup.title.text if soup.title else 'æ— '}")
        print(f"âœ… å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # æ£€æµ‹é¡µé¢æ˜¯å¦åŒ…å«ç™¾åº¦ç›¸å…³å†…å®¹
        if "ç™¾åº¦" in content:
            print("âœ… é¡µé¢åŒ…å«ç™¾åº¦ç›¸å…³å†…å®¹")
        else:
            print("âŒ é¡µé¢å¯èƒ½è¢«æ‹¦æˆªæˆ–é‡å®šå‘")
    
        # ä¿å­˜å®Œæ•´é¡µé¢åˆ°æ–‡ä»¶ä»¥ä¾¿è¯¦ç»†æ£€æŸ¥
        with open("baidu_news.html", "w", encoding="utf-8") as f:
            f.write(content)
        print("ğŸ“„ å®Œæ•´é¡µé¢å·²ä¿å­˜åˆ°baidu_news.html")
        
        # å°è¯•è§£ææ–°é—»åˆ—è¡¨
        print("\nå¼€å§‹è§£ææ–°é—»åˆ—è¡¨...")
        
        # æ£€æŸ¥content_leftå®¹å™¨
        content_left = soup.find("div", id="content_left")
        if content_left:
            print(f"âœ… æ‰¾åˆ°content_leftå®¹å™¨ï¼ŒåŒ…å« {len(content_left.find_all('div'))} ä¸ªdivå…ƒç´ ")
            
            # ä¿å­˜content_leftçš„å†…å®¹åˆ°æ–‡ä»¶ä»¥ä¾¿æ£€æŸ¥
            with open("baidu_news_content_left.html", "w", encoding="utf-8") as f:
                f.write(str(content_left))
            print("âœ… content_leftå†…å®¹å·²ä¿å­˜åˆ°baidu_news_content_left.html")
            
            # è¯¦ç»†æ£€æŸ¥content_leftä¸­çš„æ‰€æœ‰å…ƒç´ 
            print("\ncontent_leftä¸­çš„å…ƒç´ ç»“æ„:")
            for child in content_left.children:
                if child.name:
                    print(f"  - {child.name} (class: {child.get('class')}, id: {child.get('id')})")
                    # å¦‚æœæ˜¯divä¸”æœ‰å­å…ƒç´ ï¼Œç»§ç»­æ£€æŸ¥
                    if child.name == 'div' and child.children:
                        for grandchild in child.children:
                            if grandchild.name:
                                print(f"    + {grandchild.name} (class: {grandchild.get('class')}, id: {grandchild.get('id')})")
        else:
            print("âŒ æœªæ‰¾åˆ°content_leftå®¹å™¨")
            # æ£€æŸ¥bodyä¸‹çš„æ‰€æœ‰div
            body_divs = soup.body.find_all('div') if soup.body else []
            print(f"ğŸ” bodyä¸‹å…±æœ‰ {len(body_divs)} ä¸ªdivå…ƒç´ ")
            # æ‰“å°å‰10ä¸ªdivçš„ä¿¡æ¯
            for i, div in enumerate(body_divs[:10]):
                print(f"  Div {i}: id={div.get('id')}, class={div.get('class')}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°é—»æ¡ç›®
        print("\nå°è¯•ä¸åŒçš„é€‰æ‹©å™¨æŸ¥æ‰¾æ–°é—»æ¡ç›®:")
        
        # é€‰æ‹©å™¨1: .result-op.c-container
        news_items1 = soup.select(".result-op.c-container")
        print(f"1. .result-op.c-container: {len(news_items1)} ä¸ªç»“æœ")
        
        # é€‰æ‹©å™¨2: .c-container
        news_items2 = soup.select(".c-container")
        print(f"2. .c-container: {len(news_items2)} ä¸ªç»“æœ")
        
        # é€‰æ‹©å™¨3: .news-item
        news_items3 = soup.select(".news-item")
        print(f"3. .news-item: {len(news_items3)} ä¸ªç»“æœ")
        
        # é€‰æ‹©å™¨4: æ‰€æœ‰åŒ…å«h3çš„div
        news_items4 = [div for div in soup.find_all('div') if div.find('h3')]
        print(f"4. åŒ…å«h3çš„div: {len(news_items4)} ä¸ªç»“æœ")
        
        # å¦‚æœæ‰¾åˆ°åŒ…å«h3çš„divï¼Œæ‰“å°å‰3ä¸ªçš„ç»“æ„
        if news_items4:
            print("\nå‰3ä¸ªåŒ…å«h3çš„divç»“æ„:")
            for i, div in enumerate(news_items4[:3]):
                print(f"\næ–°é—»æ¡ç›® {i+1}:")
                print(f"  - div class: {div.get('class')}")
                print(f"  - div id: {div.get('id')}")
                print(f"  - æ ‡é¢˜: {div.find('h3').text.strip() if div.find('h3') else 'æ— '}")
                a_tag = div.find('h3').find('a') if div.find('h3') else None
                if a_tag:
                    print(f"  - URL: {a_tag.get('href')}")
                
                # æŸ¥æ‰¾æ¥æºä¿¡æ¯
                source_spans = div.find_all('span')
                for span in source_spans:
                    if span.text.strip() and len(span.text.strip()) < 20:
                        print(f"  - æ¥æº: {span.text.strip()}")
                
                # æŸ¥æ‰¾æ‘˜è¦
                divs = div.find_all('div')
                for d in divs:
                    if d.text.strip() and len(d.text.strip()) > 50 and len(d.text.strip()) < 200:
                        print(f"  - æ‘˜è¦: {d.text.strip()}")
        
        # æŸ¥æ‰¾é¡µé¢ä¸­çš„æ‰€æœ‰h3æ ‡ç­¾ï¼ˆé€šå¸¸æ–°é—»æ ‡é¢˜ç”¨h3ï¼‰
        h3_tags = soup.find_all('h3')
        print(f"\né¡µé¢ä¸­å…±æœ‰ {len(h3_tags)} ä¸ªh3æ ‡ç­¾")
        if h3_tags:
            print("å‰5ä¸ªh3æ ‡ç­¾çš„å†…å®¹:")
            for i, h3 in enumerate(h3_tags[:5]):
                print(f"  {i+1}. {h3.text.strip()}")
                a_tag = h3.find('a')
                if a_tag:
                    print(f"     URL: {a_tag.get('href')}")

    except requests.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("æµ‹è¯•ç™¾åº¦æ–°é—»é¡µé¢ç»“æ„ - è¯¦ç»†ç‰ˆæœ¬\n")
    test_baidu_news("ç§‘æŠ€")
